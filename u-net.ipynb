{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10067627,"sourceType":"datasetVersion","datasetId":6204892}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Neural Networks and Deep Learning\n\n---\n\n## Homework 2: Minimal Working Example","metadata":{"id":"nuwVgG3Vbbka"}},{"cell_type":"markdown","source":"## ⚙️ Import Libraries","metadata":{"id":"d7IqZP5Iblna"}},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\n\nimport keras\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom tensorflow.keras import backend as K\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ConvNeXtBase\nseed = 42\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {tfk.__version__}\")\nprint(f\"GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")","metadata":{"id":"CO6_Ft_8T56A","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:53:48.479561Z","iopub.execute_input":"2024-12-04T21:53:48.480009Z","iopub.status.idle":"2024-12-04T21:54:03.243089Z","shell.execute_reply.started":"2024-12-04T21:53:48.479938Z","shell.execute_reply":"2024-12-04T21:54:03.242013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ⏳ Load the Data","metadata":{"id":"GN_cpHlSboXV"}},{"cell_type":"code","source":"file_path = \"/kaggle/input/mars-data/mars_for_students.npz\"\n\ndata = np.load(file_path)\n\ntraining_set = data[\"training_set\"]\nimages = training_set[:, 0]\nlabels = training_set[:, 1]\n\n\nprint(f\"Training X shape: {images.shape}\")\nprint(f\"Training y shape: {labels.shape}\")","metadata":{"id":"pLaoDaG1V1Yg","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:03.244730Z","iopub.execute_input":"2024-12-04T21:54:03.245327Z","iopub.status.idle":"2024-12-04T21:54:04.571034Z","shell.execute_reply.started":"2024-12-04T21:54:03.245293Z","shell.execute_reply":"2024-12-04T21:54:04.569877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the paths (not the data) into training, validation, and test sets\nprint(\"Splitting data...\")\n# 10% of input data to both test and validation sets\ntest_ratio = 0.05 \nvalidation_ratio = 0.05 / 0.9 \n\ntrain_val_img, test_img, train_val_lbl, test_lbl = train_test_split(\n    images, labels, test_size=test_ratio, random_state=seed\n)\ntrain_img, val_img, train_lbl, val_lbl = train_test_split(\n    train_val_img, train_val_lbl, test_size=validation_ratio, random_state=seed\n)\nprint(\"Data splitted!\")\n\nprint(f\"\\nNumber of images:\")\nprint(f\"Train: {len(train_img)}\")\nprint(f\"Validation: {len(val_img)}\")\nprint(f\"Test: {len(test_img)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.572168Z","iopub.execute_input":"2024-12-04T21:54:04.572479Z","iopub.status.idle":"2024-12-04T21:54:04.863997Z","shell.execute_reply.started":"2024-12-04T21:54:04.572448Z","shell.execute_reply":"2024-12-04T21:54:04.862796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set number of classes\nNUM_CLASSES = 5\n\n# Set batch size for training\nBATCH_SIZE = 4\n\n# Set learning rate for the optimiser\nLEARNING_RATE = 1e-3\n\n# Set early stopping patience threshold\nPATIENCE = 30\n\n# Set maximum number of training epochs\nEPOCHS = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.866055Z","iopub.execute_input":"2024-12-04T21:54:04.866398Z","iopub.status.idle":"2024-12-04T21:54:04.871973Z","shell.execute_reply.started":"2024-12-04T21:54:04.866365Z","shell.execute_reply":"2024-12-04T21:54:04.870554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Add color channel and rescale pixels between 0 and 1\ntrain_img = train_img[..., np.newaxis] / 255.0\nval_img = val_img[..., np.newaxis] / 255.0\ntest_img = test_img[..., np.newaxis] / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.873188Z","iopub.execute_input":"2024-12-04T21:54:04.873544Z","iopub.status.idle":"2024-12-04T21:54:04.960839Z","shell.execute_reply.started":"2024-12-04T21:54:04.873512Z","shell.execute_reply":"2024-12-04T21:54:04.959884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.962108Z","iopub.execute_input":"2024-12-04T21:54:04.962444Z","iopub.status.idle":"2024-12-04T21:54:04.970570Z","shell.execute_reply.started":"2024-12-04T21:54:04.962411Z","shell.execute_reply":"2024-12-04T21:54:04.969388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_single_image(image, label, input_size=(64, 128)):\n    \"\"\"\n    Load a single image-label pair with the correct shape.\n    \"\"\"\n    # Read and preprocess the image\n    #image = np.repeat(image, 3, axis=-1)  # Repeat channels to make it RGB\n    #image = tf.image.resize(image, input_size) # Resize\n    #image = tf.cast(image, tf.float32) / 255.0 # Normalize\n    image = tf.image.grayscale_to_rgb(image)\n    print(image.shape)\n\n    # Read and preprocess the label\n    #label = tf.io.decode_png(label, channels=1)  # Ensure single channel\n    #label = tf.image.resize(label, input_size, method='bilinear')  # Resize to fixed size\n    label = label[..., np.newaxis] \n    label = tf.cast(label, tf.int32)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.971829Z","iopub.execute_input":"2024-12-04T21:54:04.972177Z","iopub.status.idle":"2024-12-04T21:54:04.981065Z","shell.execute_reply.started":"2024-12-04T21:54:04.972145Z","shell.execute_reply":"2024-12-04T21:54:04.980014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_map =  {\n    0: 'Background',\n    1: 'Soil',\n    2: 'Bedrock',\n    3: 'Sand',\n    4: 'Big Roc'\n}\ncategory_map = {0:0, 1:1, 2:2, 3:3, 4:4}\ndef apply_category_mapping(label):\n    \"\"\"\n    Apply category mapping to labels.\n    \"\"\"\n    keys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n    vals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n    table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n        default_value=0\n    )\n    return table.lookup(label)#@tf.function\n\ndef random_flip(image, label, seed=None):\n    \"\"\"Consistent random horizontal flip.\"\"\"\n    print(\"----\", image.shape)\n    if seed is None:\n        seed = np.random.randint(0, 1000000)\n    flip_prob = tf.random.uniform([], seed=seed)\n    image = tf.cond(\n        flip_prob > 0.5,\n        lambda: tf.image.flip_left_right(image),\n        lambda: image\n    )\n    label = tf.cond(\n        flip_prob > 0.5,\n        lambda: tf.image.flip_left_right(label),\n        lambda: label\n    )\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.982319Z","iopub.execute_input":"2024-12-04T21:54:04.982632Z","iopub.status.idle":"2024-12-04T21:54:04.995285Z","shell.execute_reply.started":"2024-12-04T21:54:04.982600Z","shell.execute_reply":"2024-12-04T21:54:04.994139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_dataset(image_paths, label_paths, batch_size, shuffle=True, augment=False, seed=None):\n    \"\"\"\n    Create a memory-efficient TensorFlow dataset.\n    \"\"\"\n    # Create dataset from file paths\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size * 2, seed=seed)\n\n    # Load images and labels\n    dataset = dataset.map(\n        load_single_image,\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    # Apply category mapping\n    dataset = dataset.map(\n        lambda x, y: (x, apply_category_mapping(y)),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    if augment:\n        dataset = dataset.map(\n            lambda x, y: random_flip(x, y, seed=seed),\n            num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # Batch the data\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:04.996522Z","iopub.execute_input":"2024-12-04T21:54:04.996858Z","iopub.status.idle":"2024-12-04T21:54:05.009446Z","shell.execute_reply.started":"2024-12-04T21:54:04.996824Z","shell.execute_reply":"2024-12-04T21:54:05.008326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the datasets\nprint(\"Creating datasets...\")\n\ntrain_dataset = make_dataset(\n    train_img, train_lbl,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    augment=True,\n    seed=seed\n)\n\nval_dataset = make_dataset(\n    val_img, val_lbl,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\ntest_dataset = make_dataset(\n    test_img, test_lbl,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\nprint(\"Datasets created!\")\n\n# Check the shape of the data\nfor images, labels in train_dataset.take(1):\n    input_shape = images.shape[1:]\n    print(f\"\\nInput shape: {input_shape}\")\n    print(\"Images shape:\", images.shape)\n    print(\"Labels shape:\", labels.shape)\n    print(\"Labels dtype:\", labels.dtype)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:05.012844Z","iopub.execute_input":"2024-12-04T21:54:05.013729Z","iopub.status.idle":"2024-12-04T21:54:06.543909Z","shell.execute_reply.started":"2024-12-04T21:54:05.013685Z","shell.execute_reply":"2024-12-04T21:54:06.542878Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_segmentation_colormap(num_classes):\n    \"\"\"\n    Create a linear colormap using a predefined palette.\n    Uses 'viridis' as default because it is perceptually uniform\n    and works well for colorblindness.\n    \"\"\"\n    return plt.cm.viridis(np.linspace(0, 1, num_classes))\n\ndef apply_colormap(label, colormap=None):\n    \"\"\"\n    Apply the colormap to a label.\n    \"\"\"\n    # Ensure label is 2D\n    label = np.squeeze(label)\n\n    if colormap is None:\n        num_classes = len(np.unique(label))\n        colormap = create_segmentation_colormap(num_classes)\n\n    # Apply the colormap\n    colored = colormap[label.astype(int)]\n\n    return colored\n\ndef plot_sample_batch(dataset, num_samples=3):\n    \"\"\"\n    Display some image and label pairs from the dataset.\n    \"\"\"\n    plt.figure(figsize=(15, 4*num_samples))\n\n    for images, labels in dataset.take(1):\n        labels_np = labels.numpy()\n        num_classes = len(np.unique(labels_np))\n        colormap = create_segmentation_colormap(num_classes)\n\n        for j in range(min(num_samples, len(images))):\n            # Plot original image\n            plt.subplot(num_samples, 2, j*2 + 1)\n            plt.imshow(images[j])\n            plt.title(f'Image {j+1}')\n            plt.axis('off')\n\n            # Plot colored label\n            plt.subplot(num_samples, 2, j*2 + 2)\n            colored_label = apply_colormap(labels_np[j], colormap)\n            plt.imshow(colored_label)\n            plt.title(f'Label {j+1}')\n            plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n# Visualize examples from the training set\nprint(\"Visualizing examples from the training set:\")\nplot_sample_batch(train_dataset, num_samples=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:06.545286Z","iopub.execute_input":"2024-12-04T21:54:06.545589Z","iopub.status.idle":"2024-12-04T21:54:07.575343Z","shell.execute_reply.started":"2024-12-04T21:54:06.545560Z","shell.execute_reply":"2024-12-04T21:54:07.574238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build the Model","metadata":{}},{"cell_type":"code","source":"'''\nReferences: \nPaper: https://arxiv.org/pdf/1707.03718\nCode: https://github.com/nickhitsai/LinkNet-Keras/blob/master/linknet.py\n'''\ndef link_net(input_shape=input_shape):\n\n    # Input Layer\n    inputs = tfkl.Input(shape=input_shape, name='input_layer')\n    # Input Block\n    x = tfkl.Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2))(inputs)\n    x = tfkl.BatchNormalization()(x)\n    x = tfkl.ReLU()(x)\n    x = tfkl.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n    skipped = []\n\n    # Parameters m and n from the paper\n    parameters_encoder = [[64, 64], [64, 128], [128, 256], [256, 512]]\n    parameters_decoder = [[64, 64], [128, 64], [256, 128], [512, 256]]\n    depth = 4\n    # Encoding\n    for i in range(depth):\n        # Encoder Block\n        # Conv1\n        skip1 = x\n        x = tfkl.Conv2D(filters=parameters_encoder[i][1],\n                        kernel_size=(3,3),\n                        strides=(2, 2),\n                        padding='same',\n                        name=\"conv1-encoder\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n        # Conv2\n        x = tfkl.Conv2D(filters=parameters_encoder[i][1],\n                        kernel_size=(3,3),\n                        padding='same',\n                        name=\"conv2-encoder\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n\n        # Connect-sum: The dimensions are not the same, so we have to equalize them ####################\n        input_shape = skip1.shape\n        residual_shape = x.shape\n        stride_width = int(round(input_shape[1] / residual_shape[1]))\n        stride_height = int(round(input_shape[2] / residual_shape[2]))\n\n        new_skip = tfkl.Conv2D(filters=residual_shape[3],\n                      kernel_size=(1, 1),\n                      strides=(stride_width, stride_height),\n                      padding=\"valid\",\n                      kernel_initializer=\"he_normal\",\n                      kernel_regularizer=l2(0.0001))(skip1)\n        skip2 = keras.layers.add([new_skip, x])\n        ################################################################################################ \n        \n        # Conv3\n        x = tfkl.Conv2D(filters=parameters_encoder[i][1],\n                        kernel_size=(3,3),\n                        padding='same',\n                        name=\"conv3-encoder\"+str(i+1))(skip2)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n        # Conv4\n        x = tfkl.Conv2D(filters=parameters_encoder[i][1],\n                        kernel_size=(3,3),\n                        padding='same',\n                        name=\"conv4-encoder\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n\n        # Connect-sum\n        # Connect-sum: The dimensions are not the same, so we have to equalize them ####################\n        input_shape = skip2.shape\n        residual_shape = x.shape\n        stride_width = int(round(input_shape[1] / residual_shape[1]))\n        stride_height = int(round(input_shape[2] / residual_shape[2]))\n\n        new_skip = tfkl.Conv2D(filters=residual_shape[3],\n                      kernel_size=(1, 1),\n                      strides=(stride_width, stride_height),\n                      padding=\"valid\",\n                      kernel_initializer=\"he_normal\",\n                      kernel_regularizer=l2(0.0001))(skip2)\n        x = keras.layers.add([new_skip, x])\n        ################################################################################################ \n\n        # Save the layer for skip connections\n        skipped.append(x)\n\n    # Decoding\n    for i in range(depth):\n        # Decoder Block\n        # Conv1\n        x = tfkl.Conv2D(filters=int(parameters_decoder[depth - i - 1][0]/4), #m\n                        kernel_size=(1,1),\n                        name=\"conv1-downsampling\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n\n        # Full Conv2\n        x = tfkl.UpSampling2D((2,2))(x)\n        x = tfkl.Conv2D(filters=int(parameters_decoder[depth - i - 1][0]/4), #m\n                        kernel_size=(3,3),\n                        padding='same',\n                        name=\"conv2-downsampling\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x) \n        x = tfkl.ReLU()(x)\n\n        # Conv3\n        x = tfkl.Conv2D(filters=parameters_decoder[depth - i - 1][1], #n\n                        kernel_size=(1,1),\n                        name=\"conv3-downsampling\"+str(i+1))(x)\n        x = tfkl.BatchNormalization()(x)\n        x = tfkl.ReLU()(x)\n\n        # Prepare Skip Connections for the NEXT BLOCK\n        if i != depth-1: #if its the last block, no skip connections\n            x = keras.layers.add([x, skipped[depth - i - 2]])\n            x = tfkl.ReLU()(x)\n\n    # Output Layers\n    # Full Conv1\n    x = tfkl.UpSampling2D((2,2))(x)\n    x = tfkl.Conv2D(filters=32,\n                    kernel_size=(3,3),\n                    strides=(1, 1),\n                    padding='same',\n                    name=\"output-fullconv1\")(x)\n    x = tfkl.BatchNormalization()(x) \n    x = tfkl.ReLU()(x)\n\n    # Conv2\n    x = tfkl.Conv2D(filters=32,\n                    kernel_size=(3,3),\n                    strides=(1, 1),\n                    padding='same',\n                    name=\"output-conv2\")(x)\n    x = tfkl.BatchNormalization()(x) \n    x = tfkl.ReLU()(x)\n\n    # Full Conv3\n    x = tfkl.UpSampling2D((2,2))(x)\n    x = tfkl.Conv2D(filters=NUM_CLASSES,\n                    kernel_size=(2,2),\n                    strides=(1, 1),\n                    padding='same',\n                    name=\"output-fullconv3\")(x)\n    x = tfkl.BatchNormalization()(x) \n    outputs = tfkl.ReLU()(x)\n    \n    model = tf.keras.Model(inputs, outputs, name='LinkNet')\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:07.576904Z","iopub.execute_input":"2024-12-04T21:54:07.577273Z","iopub.status.idle":"2024-12-04T21:54:07.600134Z","shell.execute_reply.started":"2024-12-04T21:54:07.577241Z","shell.execute_reply":"2024-12-04T21:54:07.599025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def u_net(input_shape=input_shape, depth=4, \n          d_conv_count = 3, b_conv_count = 3, u_conv_count = 3, \n          start_filter=32, skip_connections=True, dropout=0.2):\n\n    # Input Layer\n    inputs = tfkl.Input(shape=input_shape, name='input_layer')\n\n    x = inputs\n    skipped = []\n    # Downsampling\n    for i in range(depth):\n        for j in range(d_conv_count):\n            x = tfkl.Conv2D(filters=start_filter,\n                            kernel_size=(3,3),\n                            strides=(1, 1),\n                            padding='same')(x)\n            x = tfkl.BatchNormalization()(x) \n            x = tfkl.ReLU()(x)\n\n        if skip_connections:\n            # Save the layer for skip connections\n            skipped.append(x)\n\n        x = tfkl.MaxPooling2D(pool_size=(2,2))(x)\n        x = tfkl.Dropout(dropout)(x)\n        \n        start_filter = start_filter * 2\n    \n    # Bottleneck\n    for j in range(b_conv_count):\n        x = tfkl.Conv2D(filters=start_filter,\n                            kernel_size=(3,3),\n                            strides=(1, 1),\n                            padding='same')(x)\n        x = tfkl.BatchNormalization()(x) \n        x = tfkl.ReLU()(x)\n    \n    start_filter = start_filter // 2\n\n    # Upsampling\n    for i in range(depth):\n        x = tfkl.UpSampling2D(2, interpolation='bilinear')(x)\n\n        if skip_connections:\n            x = tfkl.Concatenate()([x, skipped[depth - i - 1]])\n\n        x = tfkl.Dropout(dropout)(x)\n\n        for j in range(u_conv_count):\n            x = tfkl.Conv2D(filters=start_filter,\n                            kernel_size=(3,3),\n                            strides=(1, 1),\n                            padding='same')(x)\n            x = tfkl.BatchNormalization()(x) ## remove?\n            x = tfkl.ReLU()(x)\n        start_filter = start_filter // 2\n    \n\n    # Output Layer\n    outputs = tfkl.Conv2D(filters=NUM_CLASSES,\n                        kernel_size=(1,1),\n                        strides=(1, 1),\n                        padding='same',\n                        activation='softmax',\n                        name=\"output_layer\")(x)\n    \n    model = tf.keras.Model(inputs, outputs, name='UNet')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:54:07.601442Z","iopub.execute_input":"2024-12-04T21:54:07.601839Z","iopub.status.idle":"2024-12-04T21:54:07.621997Z","shell.execute_reply.started":"2024-12-04T21:54:07.601806Z","shell.execute_reply":"2024-12-04T21:54:07.620934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nReferences:\nPaper: https://qims.amegroups.org/article/view/43519/html\nCode: https://github.com/THUHoloLab/Dense-U-net/blob/master/Dense-U-net/Dense_U_net.py\n'''\n\ndef dense_unet(input_shape=input_shape, depth=4, \n               d_conv_count = 3, u_conv_count = 3,\n               start_filter=32, dropout=0.2):\n\n    # Input Layer\n    inputs = tfkl.Input(shape=input_shape, name='input_layer')\n\n    next = inputs\n    skipped = []\n\n    # Downsampling\n    for i in range(depth):\n        # Now this is a Dense Block\n        for j in range(d_conv_count):\n            \n            x = tfkl.Conv2D(filters=start_filter,\n                            kernel_size=(3,3),\n                            kernel_initializer=\"he_normal\",\n                            strides=(1, 1),\n                            padding='same')(next)\n            x = tfkl.BatchNormalization()(x) \n            conv_block = tfkl.ReLU()(x)\n            next = tfkl.Concatenate()([conv_block, next]) \n\n        \n        skipped.append(conv_block)\n\n        x = tfkl.MaxPooling2D(pool_size=(2,2))(conv_block)\n        next = tfkl.Dropout(dropout)(x)\n        \n        start_filter = start_filter * 2\n    \n    # Bottleneck\n    x = tfkl.Conv2D(filters=start_filter,\n                        kernel_size=(3,3),\n                        strides=(1, 1),\n                        kernel_initializer=\"he_normal\",\n                        padding='same')(next)\n    x = tfkl.BatchNormalization()(x) ##### not sure\n    x = tfkl.ReLU()(x)\n    next = tfkl.Dropout(dropout)(x)\n    \n    start_filter = start_filter // 2\n\n    # Upsampling\n    for i in range(depth):\n        x = tfkl.UpSampling2D(2, interpolation='bilinear')(next)\n        x = tfkl.Concatenate()([x, skipped.pop()]) ## try without axis\n        next = tfkl.Dropout(dropout)(x)\n\n        # Dense Block again\n        for j in range(u_conv_count):\n            \n            x = tfkl.Conv2D(filters=start_filter,\n                            kernel_size=(3,3),\n                            kernel_initializer=\"he_normal\",\n                            strides=(1, 1),\n                            padding='same')(next)            \n            x = tfkl.BatchNormalization()(x) \n            conv_block = tfkl.ReLU()(x)\n            next = tfkl.Concatenate()([conv_block, next]) \n            \n        start_filter = start_filter // 2\n    \n    # Output Layer\n    outputs = tfkl.Conv2D(filters=NUM_CLASSES,\n                        kernel_size=(1,1),\n                        strides=(1, 1),\n                        padding='same',\n                        activation='sigmoid',\n                        name=\"output_layer\")(next)\n    \n    model = tf.keras.Model(inputs, outputs, name='Dense_UNet')\n    return model\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:03:10.125159Z","iopub.execute_input":"2024-12-04T22:03:10.125999Z","iopub.status.idle":"2024-12-04T22:03:10.140322Z","shell.execute_reply.started":"2024-12-04T22:03:10.125933Z","shell.execute_reply":"2024-12-04T22:03:10.139069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = u_net()\n\n# Print a detailed summary of the model with expanded nested layers and trainable parameters.\nmodel.summary(expand_nested=True, show_trainable=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:03:10.522048Z","iopub.execute_input":"2024-12-04T22:03:10.522436Z","iopub.status.idle":"2024-12-04T22:03:12.931289Z","shell.execute_reply.started":"2024-12-04T22:03:10.522406Z","shell.execute_reply":"2024-12-04T22:03:12.930312Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"# Define custom Mean Intersection Over Union metric\nclass MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n        if labels_to_exclude is None:\n            labels_to_exclude = [0]  # Default to excluding label 0\n        self.labels_to_exclude = labels_to_exclude\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # Convert predictions to class labels\n        y_pred = tf.math.argmax(y_pred, axis=-1)\n\n        # Flatten the tensors\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1])\n\n        # Apply mask to exclude specified labels\n        for label in self.labels_to_exclude:\n            mask = tf.not_equal(y_true, label)\n            y_true = tf.boolean_mask(y_true, mask)\n            y_pred = tf.boolean_mask(y_pred, mask)\n\n        # Update the state\n        return super().update_state(y_true, y_pred, sample_weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:03:36.554126Z","iopub.execute_input":"2024-12-04T22:03:36.554541Z","iopub.status.idle":"2024-12-04T22:03:36.562567Z","shell.execute_reply.started":"2024-12-04T22:03:36.554504Z","shell.execute_reply":"2024-12-04T22:03:36.561258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://stackoverflow.com/questions/65125670/implementing-multiclass-dice-loss-function\ndef dice_loss(y_true, y_pred, smooth=1e-7):\n    '''\n    Dice coefficient for X categories. Ignores background pixel label 0\n    Pass to model as metric during compile statement\n    '''\n    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=NUM_CLASSES)[...,1:])\n    y_pred_f = K.flatten(y_pred[...,1:])\n    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n    return K.mean((2. * intersect / (denom + smooth)))\n\ndef dice_loss_multiclass(y_true, y_pred):\n    '''\n    Dice loss to minimize. Pass to model as loss during compile statement\n    '''\n    return 1 - dice_loss(y_true, y_pred)\n\ndef combined_multiclass_loss(y_true, y_pred, alpha=0.5, beta=0.5):\n    dice = dice_loss_multiclass(y_true, y_pred)\n    categorical_ce = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n    return alpha * dice + beta * categorical_ce\n\n# TODO\n# boundary loss\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:10:09.301665Z","iopub.execute_input":"2024-12-04T22:10:09.302123Z","iopub.status.idle":"2024-12-04T22:10:09.311544Z","shell.execute_reply.started":"2024-12-04T22:10:09.302082Z","shell.execute_reply":"2024-12-04T22:10:09.310234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nprint(\"Compiling model...\")\nmodel.compile(\n    loss= combined_multiclass_loss,#keras.losses.CategoricalFocalCrossentropy(), #dice_loss_multiclass, #tf.keras.losses.Dice, #tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tf.keras.optimizers.AdamW(LEARNING_RATE),\n    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])]\n)\nprint(\"Model compiled!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:11:20.098556Z","iopub.execute_input":"2024-12-04T22:11:20.099900Z","iopub.status.idle":"2024-12-04T22:11:20.114470Z","shell.execute_reply.started":"2024-12-04T22:11:20.099838Z","shell.execute_reply":"2024-12-04T22:11:20.113389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🛠️ Train the Model","metadata":{"id":"FSliIxBvbs2Q"}},{"cell_type":"code","source":"# Setup callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=PATIENCE,\n    restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=3,\n    mode=\"auto\",\n    min_lr=1e-5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:11:21.056911Z","iopub.execute_input":"2024-12-04T22:11:21.057864Z","iopub.status.idle":"2024-12-04T22:11:21.063899Z","shell.execute_reply.started":"2024-12-04T22:11:21.057807Z","shell.execute_reply":"2024-12-04T22:11:21.062681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n).history\n\n# Calculate and print the final validation accuracy\nfinal_val_meanIoU = round(max(history['val_mean_iou'])* 100, 2)\nprint(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:11:21.335220Z","iopub.execute_input":"2024-12-04T22:11:21.335584Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model to a file with the accuracy included in the filename\nmodel_filename = 'model.keras'\nmodel.save(model_filename)\n\n# Delete the model to free up resources\ndel model","metadata":{"id":"PtM0ubgdOzG-","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T19:16:43.272583Z","iopub.execute_input":"2024-12-02T19:16:43.272878Z","iopub.status.idle":"2024-12-02T19:16:43.408447Z","shell.execute_reply.started":"2024-12-02T19:16:43.272850Z","shell.execute_reply":"2024-12-02T19:16:43.407729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot and display training and validation loss\nplt.figure(figsize=(18, 3))\nplt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\nplt.plot(history['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\nplt.title('Cross Entropy')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Plot and display training and validation accuracy\nplt.figure(figsize=(18, 3))\nplt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\nplt.plot(history['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\nplt.title('Accuracy')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Plot and display training and validation mean IoU\nplt.figure(figsize=(18, 3))\nplt.plot(history['mean_iou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\nplt.plot(history['val_mean_iou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\nplt.title('Mean Intersection over Union')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 📊 Prepare Your Submission\n\nIn our Kaggle competition, submissions are made as `csv` files. To create a proper `csv` file, you need to flatten your predictions and include an `id` column as the first column of your dataframe. To maintain consistency between your results and our solution, please avoid shuffling the test set. The code below demonstrates how to prepare the `csv` file from your model predictions.\n\n","metadata":{"id":"RNp6pUZuddqC"}},{"cell_type":"code","source":"X_test_submission = data[\"test_set\"]\nprint(f\"Test X shape: {X_test_submission.shape}\")\nX_test_submission = X_test_submission[..., np.newaxis] / 255.0\nX_test_submission = tf.cast(X_test_submission, tf.float32)\nX_test_submission = tf.image.grayscale_to_rgb(X_test_submission)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T19:33:38.162590Z","iopub.execute_input":"2024-12-02T19:33:38.163471Z","iopub.status.idle":"2024-12-02T19:33:40.002246Z","shell.execute_reply.started":"2024-12-02T19:33:38.163422Z","shell.execute_reply":"2024-12-02T19:33:40.001505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load UNet model without compiling\nmodel = tfk.models.load_model('/kaggle/working/model.keras', compile=False)\n\n# Compile the model with specified loss, optimizer, and metrics\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tfk.optimizers.AdamW(LEARNING_RATE),\n    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])]\n)\n\n# Print a detailed summary of the model with expanded nested layers and trainable parameters.\nmodel.summary(expand_nested=True, show_trainable=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T19:33:40.003609Z","iopub.execute_input":"2024-12-02T19:33:40.003894Z","iopub.status.idle":"2024-12-02T19:33:40.280519Z","shell.execute_reply.started":"2024-12-02T19:33:40.003868Z","shell.execute_reply":"2024-12-02T19:33:40.279434Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = model.predict(X_test_submission)\npreds = np.argmax(preds, axis=-1)\nprint(f\"Predictions shape: {preds.shape}\")","metadata":{"id":"FMIq69eWgRmr","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T19:33:40.281590Z","iopub.execute_input":"2024-12-02T19:33:40.281862Z","iopub.status.idle":"2024-12-02T19:33:49.329320Z","shell.execute_reply.started":"2024-12-02T19:33:40.281835Z","shell.execute_reply":"2024-12-02T19:33:49.328376Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def y_to_df(y) -> pd.DataFrame:\n    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n    n_samples = len(y)\n    y_flat = y.reshape(n_samples, -1)\n    df = pd.DataFrame(y_flat)\n    df[\"id\"] = np.arange(n_samples)\n    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n    return df[cols]\n\n# Create and download the csv submission file\nsubmission_filename = f\"submission.csv\"\nsubmission_df = y_to_df(preds)\nsubmission_df.to_csv(submission_filename, index=False)","metadata":{"id":"s18kX1uDconq","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T19:33:49.331133Z","iopub.execute_input":"2024-12-02T19:33:49.331496Z","iopub.status.idle":"2024-12-02T19:34:10.720681Z","shell.execute_reply.started":"2024-12-02T19:33:49.331457Z","shell.execute_reply":"2024-12-02T19:34:10.719696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}